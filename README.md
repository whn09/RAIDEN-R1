# RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward

Implementation of the RAIDEN-R1 framework for improving role-awareness in large language models through Group Relative Policy Optimization (GRPO) with Verifiable Role-Awareness Rewards (VRAR).

English | [ç®€ä½“ä¸­æ–‡](README_zh.md)

## Paper

**Title**: RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward

**Authors**: Zongsheng Wang, Kaili Sun, Bowen Wu, Qun Yu, Ying Li, Baoxun Wang

**arXiv**: https://arxiv.org/html/2505.10218v1

## Overview

RAIDEN-R1 addresses role consistency in role-playing conversational agents through:

1. **Verifiable Role-Awareness Reward (VRAR)**: Quantifiable reward mechanism for role-aware training
2. **GRPO Training**: Using Group Relative Policy Optimization to improve role consistency
3. **High-quality Dataset**: Role-aware data with Script-Based Knowledge (SBK) and Conversation Memory (CM)

## Project Structure

```
raiden-r1/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/                    # Data processing and generation
â”‚   â”‚   â”œâ”€â”€ sglang_generator.py  # SGLang local generator (10-100x faster)
â”‚   â”‚   â”œâ”€â”€ bedrock_generator.py # AWS Bedrock data generator
â”‚   â”‚   â”œâ”€â”€ language_utils.py    # Multilingual support (zh/ja/en/ko)
â”‚   â”‚   â””â”€â”€ collection.py        # Dataset management
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â””â”€â”€ grpo_trainer.py      # GRPO implementation
â”‚   â”œâ”€â”€ evaluation/              # Evaluation metrics (SBK, CM)
â”‚   â””â”€â”€ rewards/
â”‚       â””â”€â”€ vrar.py              # Verifiable Role-Awareness Rewards
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ grpo_config.yaml         # Training configuration
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ online_profiles.jsonl    # Character profiles
â”‚   â””â”€â”€ training/                # Generated training data
â”œâ”€â”€ scripts/                     # Training and generation scripts
â””â”€â”€ accelerate_config.yaml       # Multi-GPU training config
```

## Quick Start

### 1. Installation

```bash
pip install -r requirements.txt
```

**Requirements**:
- Python 3.8+
- PyTorch 2.0+
- Transformers
- 8x NVIDIA H800/H200 GPUs for training

**GPU Memory Requirements**:
- Custom GRPO: ~100GB per GPU for Qwen2.5-14B (bf16)
- OpenR1 GRPO: ~140-160GB per GPU for Qwen2.5-14B (higher due to multiple response generation)
  - Recommended: Use Qwen2.5-7B for 140GB GPUs
  - Or use fewer generations (`num_samples_per_prompt: 2` instead of 4)

### 2. Data Generation

**Recommended: SGLang + GLM-4.6** (10-100x faster than cloud APIs)

```bash
# Download GLM-4.6 model
huggingface-cli download zai-org/GLM-4.6 --local-dir /path/to/GLM-4.6

# Start SGLang server
python -m sglang.launch_server \
    --model-path /path/to/GLM-4.6 \
    --tp-size 8 \
    --port 30000 \
    --chat-template glm4 \
    --trust-remote-code

# Generate training data (Chinese by default)
python scripts/generate_data_with_sglang.py \
    --profiles_file ./data/online_profiles.jsonl \
    --num_samples_per_profile 2 \
    --include_cm \
    --language zh

# For other languages
python scripts/generate_data_with_sglang.py --language ja  # Japanese
python scripts/generate_data_with_sglang.py --language en  # English
python scripts/generate_data_with_sglang.py --language ko  # Korean
```

**Alternative: AWS Bedrock (for quick prototyping)**

```bash
python scripts/generate_data_with_bedrock.py \
    --num_samples_per_profile 2 \
    --language zh
```

### 3. Training

RAIDEN-R1 provides two training implementations:

> ğŸ“– **Detailed OpenR1 Guide**: See [OPENR1_GUIDE.md](OPENR1_GUIDE.md) for complete documentation

#### Option A: Custom GRPO Implementation (Default)

Simple and easy to customize:

```bash
# Multi-GPU training (8x H200/H800)
accelerate launch --config_file accelerate_config.yaml \
    scripts/train.py --config configs/grpo_config.yaml

# Monitor training
watch -n 1 nvidia-smi
tail -f outputs/training.log
```

#### Option B: OpenR1 GRPO (Paper-Accurate â­)

Uses Hugging Face's OpenR1 library as mentioned in the paper:

```bash
# Install OpenR1 (must be from source)
git clone https://github.com/huggingface/open-r1.git
cd open-r1
pip install -e ".[dev]"
cd ..

# Verify installation
python scripts/test_openr1_integration.py

# Train with OpenR1 (single GPU)
python scripts/train_with_openr1.py configs/openr1_config.yaml

# Or with command-line arguments
python scripts/train_with_openr1.py \
    --train_data_path ./data/training/train.json \
    --eval_data_path ./data/training/validation.json \
    --model_name_or_path Qwen/Qwen2.5-14B-Instruct \
    --output_dir ./outputs_openr1 \
    --num_train_epochs 1 \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 2 \
    --learning_rate 3e-6

# Multi-GPU training (recommended, 8x H200/H800)
accelerate launch --config_file accelerate_config.yaml \
    scripts/train_with_openr1.py configs/openr1_config.yaml

# Monitor training
watch -n 1 nvidia-smi
tail -f outputs_openr1/training.log
```

**Comparison**:
- **Custom GRPO**: Simpler, easier to debug, good for experimentation
- **OpenR1**: Matches paper implementation, optimized performance, community-maintained

**Training Configuration** (`configs/grpo_config.yaml` or `configs/openr1_config.yaml`):
- Base Model: `Qwen/Qwen2.5-14B-Instruct`
- Learning Rate: `3e-6` with cosine scheduler
- Batch Size: `4` per GPU (effective: 64 with 8 GPUs)
- Precision: `bf16`
- Epochs: `1`
- GRPO Samples: `4` per prompt

### 4. Evaluation

```bash
python scripts/evaluate.py \
    --model_path ./outputs/epoch_0 \
    --eval_data ./data/training/validation.json \
    --output_file ./evaluation_results.json
```

## Data Generation Details

### Why GLM-4.6?

- âœ… **Excellent role-playing quality**
- âœ… **Fast inference with SGLang**
- âœ… **Supports thinking mode control** (clean outputs)
- âœ… **Native Chinese support**
- âœ… **Open-source and free**

### Supported Models (SGLang)

- **GLM-4.6** (Recommended) - Best balance of speed and quality
- **MiniMax M2** - Strong for role-playing
- **Qwen2.5-14B/32B** - Strong reasoning
- **DeepSeek-V2.5** - High quality

### Multilingual Support

RAIDEN-R1 supports multiple languages with auto-detection:

- **Chinese (zh)**: Default, native support âœ“
- **Japanese (ja)**: æ—¥æœ¬èªå¯¾å¿œ
- **English (en)**: Full support
- **Korean (ko)**: í•œêµ­ì–´ ì§€ì›

```bash
# Use specific language
python scripts/generate_data_with_sglang.py --language zh

# Auto-detect from character profiles
python scripts/generate_data_with_sglang.py --auto_detect
```

### Generation Parameters

```bash
python scripts/generate_data_with_sglang.py \
    --profiles_file ./data/online_profiles.jsonl \
    --output_file ./data/generated_samples.json \
    --num_samples_per_profile 2 \
    --include_cm \
    --language zh \
    --max_profiles 100  # Limit for testing
```

### Input Data Format

Character profiles should be in JSONL format (`data/online_profiles.jsonl`). Each line contains a character with structured information:

```jsonl
{"prompt": "...[è§’è‰²æ‰®æ¼”ç³»ç»Ÿæç¤ºè¯]...\n\n<Character Setting>\nName: å°æ˜\nGender: male\nIntroduction: ä¸€ä¸ªé˜³å…‰å¼€æœ—çš„é«˜ä¸­ç”Ÿï¼Œå–œæ¬¢æ‰“ç¯®çƒå’Œç”»ç”»ã€‚\nDetailed Description: å°æ˜ä»Šå¹´17å²ï¼Œå°±è¯»äºæŸé«˜ä¸­äºŒå¹´çº§ã€‚æ€§æ ¼å¤–å‘æ´»æ³¼ï¼Œæ€»æ˜¯èƒ½ç»™å‘¨å›´çš„äººå¸¦æ¥æ¬¢ç¬‘ã€‚è¯¾ä½™æ—¶é—´å–œæ¬¢å’Œæœ‹å‹ä¸€èµ·æ‰“ç¯®çƒï¼Œä¹Ÿçƒ­çˆ±ç»˜ç”»åˆ›ä½œã€‚æ¢¦æƒ³æ˜¯æˆä¸ºä¸€åèŒä¸šæ’ç”»å¸ˆã€‚\n</Character Setting>\n\n...[å…¶ä»–è®¾å®š]..."}
```

**Key Fields**:
- `Name`: Character name
- `Gender`: male/female
- `Introduction`: Brief introduction (1-2 sentences)
- `Detailed Description`: Detailed character background, personality, goals, etc.

The system will automatically parse the `<Character Setting>` block and generate training data based on this information.

## Training Data Format

```json
{
  "character_name": "è§’è‰²åç§°",
  "character_profile": {
    "Name": "è§’è‰²åç§°",
    "Gender": "female",
    "Introduction": "ç®€ä»‹",
    "Detailed Description": "è¯¦ç»†æè¿°"
  },
  "conversation_history": [],
  "question": "ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ",
  "answer": "æˆ‘å«å°æ˜ã€‚",
  "keywords": ["å°æ˜"],
  "question_type": "what",
  "validation_method": "single_term_validation",
  "difficulty": "easy",
  "metadata": {
    "source": "sglang_sbk",
    "model": "GLM-4.6",
    "language": "zh"
  }
}
```

## Results

The RAIDEN-R1 14B-GRPO model achieves:
- **SBK (Script-Based Knowledge)**: 88.04%
- **CM (Conversation Memory)**: 88.65%

## Troubleshooting

### Low GPU Utilization
```bash
# Ensure using accelerate for multi-GPU
accelerate launch --config_file accelerate_config.yaml scripts/train.py --config configs/grpo_config.yaml

# Increase batch size in configs/grpo_config.yaml
batch_size: 8
gradient_accumulation_steps: 2
```

### Out of Memory (OOM)
```bash
# Reduce batch size in configs/grpo_config.yaml
batch_size: 4

# Use fewer GPUs
accelerate launch --num_processes=4 scripts/train.py --config configs/grpo_config.yaml
```

### SGLang Thinking Mode Issues

If the model outputs thinking tags (`<think>...</think>`), ensure:
1. SGLang server started with `--chat-template glm4`
2. Using latest code with OpenAI SDK integration
3. Not using `--enable_thinking` flag

## Advanced Configuration

### SGLang Server Optimization

```bash
# For production with 8 GPUs
python -m sglang.launch_server \
    --model-path /path/to/GLM-4.6 \
    --tp-size 8 \
    --ep-size 8 \
    --port 30000 \
    --chat-template glm4 \
    --mem-fraction-static 0.85 \
    --trust-remote-code
```

### Training with Custom Data

```bash
# Generate more samples per profile
python scripts/generate_data_with_sglang.py \
    --num_samples_per_profile 5 \
    --include_cm

# Use custom training/validation split
python scripts/generate_data_with_sglang.py \
    --train_ratio 0.8  # 80% train, 20% validation
```

## Citation

```bibtex
@article{wang2025raiden,
  title={RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward},
  author={Wang, Zongsheng and Sun, Kaili and Wu, Bowen and Yu, Qun and Li, Ying and Wang, Baoxun},
  journal={arXiv preprint arXiv:2505.10218},
  year={2025}
}
```

## Acknowledgments

- Open-R1 library
- RAIDEN Benchmark
- Qwen2.5-14B-Instruct base model
- SGLang framework

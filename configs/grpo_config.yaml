# GRPO Training Configuration for RAIDEN-R1

# Model Configuration
model_name: "Qwen/Qwen2.5-14B-Instruct"  # Base model

# Training Hyperparameters
learning_rate: 3.0e-6
batch_size: 4  # Per GPU
num_epochs: 1
gradient_accumulation_steps: 1
max_length: 2048

# GRPO Specific
num_samples_per_prompt: 4  # Number of responses per prompt for group sampling
kl_penalty: 0.1  # KL divergence penalty coefficient

# Generation Parameters
temperature: 0.7
top_p: 0.9

# Reward Weights
accuracy_weight: 0.7  # Weight for accuracy reward
format_weight: 0.3    # Weight for format reward

# Hardware/Performance
use_bf16: true
gradient_checkpointing: true

# Output and Logging
output_dir: "./outputs"
save_steps: 100
logging_steps: 10
eval_steps: 50
warmup_steps: 100

# Dataset
train_data: "./data/train_dataset.json"
eval_data: "./data/eval_dataset.json"

# Evaluation Weights
sbk_weight: 0.5  # Script-Based Knowledge weight
cm_weight: 0.5   # Conversation Memory weight
